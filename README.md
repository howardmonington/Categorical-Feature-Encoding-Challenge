# Categorical Feature Encoding Challenge

#### -- Project Status: [Completed]

## Project Intro/Objective
The purpose of this project is to experiment with different types of categorical features.


### Methods Used
* Machine Learning
* Data Visualization (matplotlib)
* Label Encoding
* One Hot Encoding
* ROC AUC Score

### Technologies
* Python
* SGD, random_forest, extra_trees, KNeighbors, bernoulliNB, xgb
* pandas, numpy
* jupyter
* StratifiedShuffleSplit
* RandomOverSampler

## Project Description
(Provide more detailed overview of the project.  Talk a bit about your data sources and what questions and hypothesis you are exploring. What specific data analysis/visualization and modelling work are you using to solve the problem? What blockers and challenges are you facing?  Feel free to number or bullet point things here)
This project presented the challenge of encoding various different types of features, including:
* binary features
* low- and high-cardinality nominal features
* low- and high-cardinality ordinal features

I worked differently with each of these types of features in order to perform feature engineering. Finally, I tested various different types of algorithms using 5 fold cross validation in order to find the best performing model. Finally, I submitted my predictions to kaggle and achieved a final score of 0.70188. Top leaderboard scores are up to 0.8.

### Dataset Location
The Raw Data can be found [here](https://www.kaggle.com/c/dont-overfit-ii)

## Featured Notebooks/Analysis/Deliverables
* [Notebook](https://github.com/lukemonington/Categorical-Feature-Encoding-Challenge/blob/master/categorical-feature-encoding-challenge.ipynb)


## Contributing Members

**[Luke Monington](https://github.com/lukemonington)**

## Contact
* I can be reached at lukemonington@aol.com.
